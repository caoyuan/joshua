\documentclass[11pt]{article}
\usepackage{naaclhlt2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{array}
\usepackage{multirow}
\usepackage{rotating}


\author{Juri Ganitkevitch, Benjamin Van Durme \and
  Chris Callison-Burch \\
  Department of Computer Science, Johns Hopkins University \\
  Baltimore, MD 21218, USA}

\title{Monolingual Distributional Similarity for Text to Text Generation}

\begin{document}
\maketitle

\begin{abstract}
  Previous work obtained collections of paraphrases by either relying
  on sentence-aligned parallel datasets, or by using distributional
  similarity metrics over large text corpora. Our approach combines
  these two orthogonal sources of information by directly integrating
  them into the decoding algorithm. We hope to report significant
  improvements in output quality on an array of text-to-text
  generation tasks.
\end{abstract}

\section{Introduction}

A wide variety of applications in natural language processing can be
cast in terms of text-to-text generation. Given input in the form of
natural language, a text-to-text generation system produces natural
language output that fulfills previously defined constraints and
objectives on both the text's surface form and meaning. Paraphrases,
i.e.\ differing textual realizations of the same meaning, are a
crucial components of text-to-text generation systems, and have been
successfully applied to tasks such as multi-document summarization,
query expansion, question answering, sentence compression and
simplification
\cite{Barzilay1999,BarzilayThesis,mckeown:1979:ACL,Anick1999,Ravichandran2002,Riezler2007}.

Recently, \newcite{Ganitkevitch2011} presented an approach for
sentential paraphrasing derived from state-of-the-art statistical
machine translation systems. They described a large-scale extraction
method for syntactically annotated paraphrases from bilingual parallel
corpora, as well as an adaptation framework that allowed for
straight-forward, non-naive adaptation of the system to any given
sentential text-to-text generation task.

In this paper, we describe an extension of
\newcite{Ganitkevitch2011}'s approach by introducing a new component
into the paraphrasing system that is not derived from the statistical
machine translation domain and present an orthogonal source of
information: monolingual distributional similarity. More specifically,
we show that:

\begin{itemize}

\item Using monolingual distributional similarity features improves
  paraphrase quality past what we can achieve with features estimated
  from bilingual data. We demonstrate that different types of
  monolingual distributional information can be used to achieve
  differing effects such as improvements in grammaticality or word
  sense disambiguation, and discuss the trade-off between data sources
  with high-coverage versus smaller, more richly annotated corpora.

\item We define the notion of distributional similarity for paraphrase
  patterns that contain multi-word gaps. This generalizes over
  previous approaches that defined the notion for contiguous phrases
  or single-word gaps.

\item Finally, we compare the effectiveness of out method against a
  variety of baselines on an example text-to-text generation task,
  sentence compression. We show improvements in quality over both a
  purely bilingually sourced paraphrasing system and an ILP-based
  compression model.
\end{itemize}

In the following, we will give an overview of SCFG-based paraphrase
extraction (Section~\ref{sec-scfgs}) and monolingual distributional
similarity (Section~\ref{sec-mds}). Section~\ref{sec-scoring} presents
our rescoring model. We discuss the reranking results in
Section~\ref{sec-ranking}. We relate our work to prior research in
Section~\ref{sec-related-work}. Finally, Sections~\ref{sec-setup} and
\ref{sec-results} present our experimental setup and the results
obtained for the sentence compression task. We conclude in
Section~\ref{sec-conclusion}.

\section{Synchronous Context-Free Grammars}
\label{sec-scfgs}

\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.99\linewidth]{figures/syntactic_pivoting.pdf}
\end{center}
\caption{An example of syntactic paraphrase extraction and feature
  estimation via the pivoting approach.}\label{fig-syntactic-pivoting}
\end{figure}

Following \newcite{Ganitkevitch2011}, we formulate our paraphrases as
a syntactically annotated \emph{ synchronous context-free grammar}
(SCFG) \cite{Aho1972,Chiang2005}.  An SCFG rule has the form:
\begin{equation*}
  \mathbf{r} = C \rightarrow \langle f, e, \sim, \vec{\varphi} \rangle ,
\end{equation*}
where the left-hand side of the rule, $C$, is a nonterminal and the
right-hand sides $f$ and $e$ are strings of terminal and nonterminal
symbols with an equal number of nonterminals. The function $\sim$
defines a one-to-one correspondency function between the nonterminals
in $f$ and $e$. Drawing on machine translation terminology, we refer
to $f$ as the \emph{source} and $e$ as the \emph{target}
side of the rule.

Each rule is annotated with a vector of feature functions
$\vec{\varphi} = \{\varphi_1 ... \varphi_N \}$ that, using a
corresponding weight vector $\vec{\lambda}$, are combined in a
log-linear model to compute the \emph{cost} of applying $\mathbf{r}$:
\begin{equation}
  \mathit{cost}(\mathbf{r}) = -\sum_{i=1}^N \lambda_i \log \varphi_i .
\end{equation}
Typical features used in the statistical machine translation models
that our system builds on are conditional phrasal, lexical and
left-hand side label probabilities, as well as a variety of count and
indicator features. We detail the feature set used in our experiments
in Section~\ref{sec-setup}.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.99\linewidth]{figures/example_compression.pdf}
\end{center}
\caption{An example of a synchronous paraphrastic derivation.}
\label{fig-example-compression}
\end{figure}

To obtain a paraphrase grammar, we first must extract a translation
grammar that translates any given foreign language into English. Then,
for each pair of translation rules where the left-hand side $C$ and
foreign string $f$ match:
\begin{eqnarray*}
    \mathbf{r}_1 = C \rightarrow \langle f, e_1, \sim_1, \vec{\varphi}_1
  \rangle \phantom{,} \\
  \mathbf{r}_2 = C \rightarrow \langle f, e_2, \sim_2, \vec{\varphi}_2
  \rangle ,
\end{eqnarray*}
we use the intuition that two english strings $e_1$ and $e_2$ that
translate to the same foreign string $f$ are equivalent in meaning,
and \emph{pivot} over $f$ to create a paraphrase rule
\cite{Ganitkevitch2011,Callison-Burch2008,Callison-Burch2005}:
\begin{equation*}
  \mathbf{r}_p = C \rightarrow \langle e_1, e_2, \sim_p, \vec{\varphi}_p \rangle ,
\end{equation*}
with a combined nonterminal correspondency function $\sim_p$.
Similarly, the paraphrase feature vector $\vec{\varphi}_p$ is computed
from the translation feature vectors $\vec{\varphi}_1$ and
$\vec{\varphi}_2$ by following the pivoting idea. For instance, we
estimate the conditional paraphrase probability $p(e_2 | e_1)$ by
marginalizing over all shared foreign-language translations $f$:
\begin{eqnarray}
  p(e_2|e_1) &=& \sum_f p(e_2,f|e_1)\\
  &=& \sum_f p(e_2|f,e_1) p(f|e_1) \\
  &\approx& \sum_f p(e_2|f) p(f|e_1) .
\label{eq-paraphrase-probability}
\end{eqnarray}
Figure~\ref{fig-syntactic-pivoting} illustrates syntax-constrained
pivoting and feature aggregation over multiple foreign language
translations for a paraphrase pattern.
Figure~\ref{fig-example-compression} shows an example for a
synchronous paraphrastic derivation produced as a result of applying
our grammar in the decoding process.

The approach we outlined in this section relies on supervised
sentence-level parallelism to identify phrases and patterns that are
equivalent in meaning. When extracting paraphrases from monolingual
text, we have to rely on an entirely different set of semantic cues
and features.

\section{Monolingual Distributional Similarity}
\label{sec-mds}

In absence of other correspondency information, paraphrase extraction
from monolingual corpora relies on contextual features. To describe a
phrase $e$, we define a set of features that describe the context of
an occurrence of $e$ in our corpus. The resulting feature vectors
$\vec{s}_{e,i}$ are aggregated over all occurrences of $e$, resulting
in a \emph{distributional} signature for $e$, $\vec{s}_e = \sum_i
\vec{s}_{e,i}$.  Following the intuition that phrases with similar
meanings occur in similar contexts, we can then identify $e'$ as a
paraphrase of $e$ by computing the cosine similarity between their
distributional signatures:
\begin{equation*}
  \mathit{sim}(e, e') = \frac{\vec{s}_e \cdot \vec{s}_{e'}}{|\vec{s}_e||\vec{s}_{e'}|}.
\end{equation*}

The features used to describe the context of a phrase differ by
application and data source. Both \newcite{Lin2001} and
\newcite{ChurchHanks91} use a rich feature set based on constituency
and dependency parses of the text corpora they extract paraphrases
from. In their work, a phrase is described by the various syntactic
relations it has with lexical items in its context, such as the set of
verbs it appears is seen as the subject of, or the set of adjectives
that modify it. 

However, when moving to vast text collections or collapsed
representations of large text corpora, parsing can become impractical
or even impossible. In these cases using simple features based on
lexical $n$-grams has proven to be effective
\cite{LapataKellerSaLP05,Bhagat2008,LinEtAlLREC10,VanDurmeLallACL10}.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.99\linewidth]{figures/ngram_context.pdf}
\end{center}
\caption{An example of immediate lexical context acquisition over
  n-gram corpora.}\label{fig-ngram-context}
\end{figure}

In order to investigate the impact of the feature set used, we chose
to extract two collections of distributional similarity-based
paraphrases. Using a web-scale $n$-gram corpus
\cite{GoogleNgrams,LinEtAlLREC10}, we extract unigram features for the
words to the left and right for phrases up to a length of 4. The
features are weighed with the $n$-gram count given by the dataset. The
resulting collection comprised context vectors for the 200 million
most frequent 1- to 4-grams in the dataset.

For contrast, we use the constituency- and dependency-parsed Los
Angeles Times/Washington Post portion of the Gigaword corpus
\cite{Gigaword}. The following feature set is used to compute phrase
contexts over this dataset:
\begin{itemize}
\item Lexical and part-of-speech unigram and bigram features,
  drawn from a three-word window to the right and left of the phrase. 
\item Features based on dependencies for both links into and out of
  the phrase, labeled with the corresponding lexical item and POS. If
  the phrase is syntactically well-formed we additionally include
  lexical and POS features for its head.
\item Syntactic features for constituents governing the phrase, as
  well as for CCG-style slashed constituent labels for the phrase,
  split by governing constituent and missing constituent. 
\end{itemize}
Figure~\ref{fig-rich-context} illustrates our choice of feature
set. As a result we obtain context information for over 12 million 1-
to 4-gram phrases.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.99\linewidth]{figures/rich_context.pdf}
\end{center}
\caption{An example of rich monolingual distributional features over a
parsed text corpus.}\label{fig-rich-context}
\end{figure}

Much like \newcite{Ravichandran2005} and \newcite{Bhagat2008}, we
relied on Locality Sensitive Hashing (LSH), to make the use of these
large collections practical. In order to avoid explicitly computing
the feature vectors, which can be memory intensive for frequent
phrases, we chose the online LSH variant described in
\cite{VanDurmeLallACL10}. This method, based on the earlier work of
\newcite{IndykSTOC98} and \newcite{Charikar02}, approximates the
cosine similarity between two feature vectors based on the Hamming
distance in a dimensionality-reduced bitwise representation. Two
feature vectors $u$, $v$ each of dimension $d$ are first projected
through a $d \times b$ random matrix populated with draws from
$\mathcal{N}(0,1)$. We then convert the resulting $b$-dimensional
vectors into bit-vectors by setting each bit of the signature
conditioned on whether the corresponding projected value is less than
0. Now, given the bit signatures $h(\vec{u})$ and $h(\vec{v})$, we
approximate the cosine similarity of $u$ and $v$ as:
\begin{equation*}
  \mathit{sim'}(u, v) =
  \cos\Big(\frac{D(h(\vec{u}),h(\vec{v}))}{b}\pi\Big) ,
\end{equation*}
where $D()$ is the Hamming distance.



\section{Incorporating Distributional Similarity}
\label{sec-scoring}

In order for us to make the distributional similarity information
accessible to the paraphrasing system, we need to calculate similarity
scores for the paraphrastic SCFG rules in our grammar. However, the
similarity information we obtain from our monolingual sources is
defined for lexical $n$-grams only, not for patterns with gaps in
them.  

We close this gap, by decomposing the discontinuous patterns that make
up the right-hand sides of a rule $\mathbf{r}$ into pairs of
contiguous phrases $P(\mathbf{r}) = (\bar{e}, \bar{e}')$, for which we
can look up distributional signatures and compute similarity
scores. This decomposition into phrases is non-trivial, since our
sentential paraphrase rules often involve significant reordering or
structural changes. To avoid comparing unrelated phrase pairs, we
require $P(\mathbf{r})$ to be consistent with a token alignment
$\mathbf{a}$. We define and compute $\mathbf{a}$ analogously to the
word alignments used in statistical machine
translation. Figure~\ref{fig-pattern-scoring} shows an aligned rule
and the phrase pairs we extract from it.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=0.70\linewidth]{figures/pattern_scoring.pdf}
\end{center}
\caption{Scoring a rule by extracting and scoring contiguous phrases
  consistent with the alignment.}\label{fig-pattern-scoring}
\end{figure}

We define the overall similarity score of the rule to be the average
of the similarity scores of all extracted phrase pairs:
\begin{equation*}
  \mathit{sim}(\mathbf{r}, \mathbf{a}) = \frac{1}{|P(\mathbf{a})|}
  \sum_{(e, e') \in P(\mathbf{a})}\mathit{sim}(e, e') .
\end{equation*}
This is done to achieve a sparsity-reducing effect similar to backing
off in statistical $n$-gram language models. Since the distributional
signatures for long, rare phrases may be computed from only a handful
of occurrences, we additionally query for the shorter sub-phrases that
are more likely to have been observed often enough for reliable
signature and thus similarity estimates.

The similarity score is incorporated into our paraphraser as an
additional rule-level feature $\varphi_{\mathit{sim}}$, and the
corresponding weight $\lambda_{\mathit{sim}}$ is estimated along with
the other $\lambda_i$ in our training framework, detailed in
Section~\ref{sec-setup}. We keep track of the $n$-gram-based
similarity scores and the richer feature set separately.


\section{Paraphrase Ranking}
\label{sec-ranking}

We investigate the effect of combining distributional similarity 

\begin{table*}[t]
\small
\begin{center}
\begin{tabular}{|cl|cl|cl|cl|}
  \hline
  \multicolumn{8}{|c|}{phrase!} \\
  \hline
  baseline 1 & 0.96 &
  ngrams 1 & 0.9 &
  rich 1 & 0.96 &
  combined 1 & 0.9 \\

  baseline 2 & 0.96 &
  ngrams 2 & 0.9 &
  rich 2 & 0.96 &
  combined 2 & 0.9 \\

  baseline 3 & 0.96 &
  ngrams 3 & 0.9 &
  rich 3 & 0.96 &
  combined 3 & 0.9 \\


  baseline 4 & 0.96 &
  ngrams 4 & 0.9 &
  rich 4 & 0.96 &
  combined 4 & 0.9 \\


  baseline 5 & 0.96 &
  ngrams 5 & 0.9 &
  rich 5 & 0.96 &
  combined 5 & 0.9 \\

  \hline
  \multicolumn{8}{c}{} \\
  \hline
  \multicolumn{8}{|c|}{different phrase!} \\
  \hline
  baseline 1 & 0.96 &
  ngrams 1 & 0.9 &
  rich 1 & 0.96 &
  combined 1 & 0.9 \\

  baseline 2 & 0.96 &
  ngrams 2 & 0.9 &
  rich 2 & 0.96 &
  combined 2 & 0.9 \\

  baseline 3 & 0.96 &
  ngrams 3 & 0.9 &
  rich 3 & 0.96 &
  combined 3 & 0.9 \\


  baseline 4 & 0.96 &
  ngrams 4 & 0.9 &
  rich 4 & 0.96 &
  combined 4 & 0.9 \\


  baseline 5 & 0.96 &
  ngrams 5 & 0.9 &
  rich 5 & 0.96 &
  combined 5 & 0.9 \\

  \hline
\end{tabular}
\end{center}
\normalsize
\caption{Comparison of paraphrase rankings for }
\label{grammar_stats}
\end{table*}


\section{Experimental Setup}
\label{sec-setup}

\begin{table}
\small
\begin{center}
\begin{tabular}{|c|r|}
  \hline
  Grammar & \multicolumn{1}{c|}{\# Rules} \\
  \hline
  total & 42,353,318 \\
  w/o identity & 23,641,016 \\
  w/o complex constituents & 6,439,923 \\
  w/o complex const.\ \& identity & 5,097,250 \\
  \hline
\end{tabular}
\end{center}
\normalsize
\caption{Number and distribution of rules in our paraphrase
  grammar. Note the significant number of identity paraphrases and
  rules with complex nonterminal labels.}
\label{grammar_stats}
\end{table}

We extracted our paraphrase grammar from the French--English portion
of the Europarl corpus (version 5). The Berkeley aligner and the
Berkeley parser were used to align the bitext and parse the English
side, respectively. The paraphrase grammar was produced using the
Hadoop-based Thrax grammar extractor's paraphrase mode. The syntactic
nonterminal labels we allowed in the grammar were limited to
constituent labels and CCG-style slashed categories. Paraphrase
grammars extracted via pivoting tend to grow very large. To keep the
grammar size manageable, we pruned away all paraphrase rules whose
phrasal paraphrase probabilities $p(e_1|e_2)$ or $p(e_2|e_1)$ were
smaller than $0.001$.

We extended the feature set used by \newcite{Ganitkevitch2011} with a
number of features that aim to better describe a rule's compressive
power: on top of the word count features $c_{\mathit{src}}$ and
$c_{\mathit{tgt}}$ and the word count difference feature
$c_{\mathit{diff}}$, we add character based count and difference
features $\mathit{char}_{\mathit{src}}$,
$\mathit{char}_{\mathit{tgt}}$, and $\mathit{char}_{\mathit{diff}}$,
as well as log-compression ratio features $c_{\mathit{cr}} = \log
\frac{c_{\mathit{tgt}}}{c_{\mathit{src}}}$ and the analogously defined
$\mathit{char}_{\mathit{cr}}$.

For model tuning and decoding, we used the Joshua machine translation
system \cite{Joshua-3.0}. The model weights were estimated using an
implementation of the PRO tuning algorithm \cite{PRO2011}, with
\textsc{Pr\'ecis} as our objective function \cite{Ganitkevitch2011}.  The
language model used in our paraphraser and the \newcite{Clarke2008}
baseline system is a Kneser-Ney discounted 5-gram model estimated on
the Gigaword corpus using the SRILM toolkit \cite{SRILM}.


\section{Sentence Compression Results}
\label{sec-results}

We adopt the text-to-text evaluation task used by
\newcite{Ganitkevitch2011} and assess the output quality of our
paraphrasing system by using it to perform sentence compression. We
contrast our distributional similarity-informed paraphrase system with
an uninformed pivoting-only baseline, as well as an implementation of
\newcite{Clarke2008}'s state-of-the-art compression model which uses a
series of constraints in an integer linear programming (ILP) solver.

%TODO: start back up here.

We solicit human judgments of the compressions along two five-point
scales: grammaticality and meaning. Judges are instructed to decide
how much the meaning from a reference translation is retained in the
compressed sentence, with a score of 5 indicating that all of the
important information is present, and 1 being that the compression
does not retain any of the original meaning. Similarly, a grammar
score of 5 indicates perfect grammaticality, and a grammar score of 1
is assigned to sentences that are entirely ungrammatical. To ensure
fairness, we perform pairwise system comparisons with compression
rates strictly tied on the sentence-level. For any comparison, a
sentence is only included in the computation of average scores if the
difference between both systems' compression rates is
$<0.05$.\footnote{Because evaluation quality correlates linearly with
  compression rate, the community-accepted practice of not comparing
  based on a closely tied compression rate is potentially subject to
  erroneous interpretation \cite{Napoles2011}.}

Table~\ref{comparison} shows a set of pairwise comparisons for
compression rates $\approx 0.5$. We see that going from a Hiero-based
to a syntactic paraphrase grammar yields a significant improvement in
grammaticality. Adding compression-specific features improves
grammaticality even further. Further augmenting the grammar with
deletion rules significantly helps retain the core meaning at
compression rates this high, however compared to the un-augmented
syntactic system grammaticality scores drop. While our approach
significantly outperforms the T3 system, we are not able to match
ILP's results in grammaticality.


In Table~\ref{human_judgments} we compare our system to the ILP
approach at a modest compression rate of $\approx 0.8$.  Here, we
significantly outperform ILP in meaning retention while achieving
comparable results in grammaticality.

These results indicate that, over a variety of compression rates, our
framework for text-to-text generation is performing as well as or
better than specifically tailored state-of-the-art methods.

Table~\ref{test_examples} shows an example sentence drawn from our
test set and the compressions produced by the different systems. We
see that both the paraphrase and ILP systems produce good quality
results, with the paraphrase system retaining the meaning of the
source sentence more accurately.


 \begin{table}
   \small
   \begin{center}
     \begin{tabular}{|c|c|c|c|}
       \hline
       & CR & Meaning & Grammar \\
       \hline
       Reference & 0.73 &  4.80 & 4.53 \\
       \hline
       \hline
       Paraphrase + $n$-gram & 0.78 & {\bf 3.85} & 3.28 \\
       ILP & 0.78 & 3.58 & {\bf 3.51} \\
       Paraphrase & 0.78 & 3.73 & 3.13 \\
       \hline
       \hline
       Random Deletions & 0.78 & 3.19 & 2.57 \\
       \hline
     \end{tabular}
   \end{center}
   \normalsize
   \caption{Results of the human evaluation on longer compressions:
     pairwise compression rates (CR), meaning and grammaticality scores. 
     Bold indicates a statistically significance difference at $p <
     0.05$.}
   \label{human_judgments}
 \end{table}

\begin{table}
   \small
   \begin{center}
     \begin{tabular}{|l|c|c|c|}
       \hline
       & CR & Meaning & Grammar \\
       \hline
       Paraphrase + $n$-gram & 0.77 & 3.79 & {\bf 3.16} \\
       Paraphrase & 0.77 & 3.78 & 3.08 \\
       \hline
       \hline
       Paraphrase + $n$-gram & 0.76 & 3.83 & 3.23 \\
       ILP & 0.76 & 3.70 & 3.58 \\
       \hline
    \end{tabular}
   \end{center}
   \normalsize
   \caption{Human evaluation for shorter compressions and for
     variations of our paraphrase system.  +Feat.\ includes the
     compression features from Section \ref{feat-design}, +Aug.\
     includes optional deletion rules from Section
     \ref{injection}. \vspace{-.5cm}}
   \label{comparison}
 \end{table}

\section{Conclusion}
\label{sec-conclusion}

In this work we introduced a method to learn syntactically informed
paraphrases from bilingual parallel texts. We discussed the expressive
power and limitations of our formalism and outlined straightforward
adaptation strategies for applications in text-to-text generation. We
demonstrated when our paraphrasing system was adapted to do sentence
compression, it achieved results competitive with state-of-the-art
compression systems with only minimal effort.


\bibliographystyle{naaclhlt2012}
\bibliography{monods_t2t}
\end{document}