### Part 1: parameters similar to Z-MERT
# target sentence file name (in this case, file name prefix)
-r	 ./ref.en
-rps	 4			# references per sentence
-p	 ./params.txt	  	# parameter file

#metric setting:
-m	 BLEU 4 closest
#-m	 TER nocase punc 5 5 joshua/zmert/tercom-0.7.25/tercom.7.25.jar 1
#-m	 TER-BLEU nocase punc 20 50  joshua/zmert/tercom-0.7.25/tercom.7.25.jar 1 4 closest
#-m	 METEOR en norm_yes keepPunc 2  #old meteor interface  #Z-MERT Meteor interface(not working)
#-m	 Meteor en lowercase '0.5 1.0 0.5 0.5' 'exact stem synonym paraphrase' '1.0 0.5 0.5 0.5' #CMU meteor interface

-maxIt	 20	   	     # maximum MIRA iterations
-cmd	 ./decoder_command   # file containing commands to run decoder
-decOut	 ./output.nbest      # file prodcued by decoder
-dcfg	 ./joshua.config     # decoder config file
-N	 500                 # size of N-best list
-v	 1                   # verbosity level (0-2; higher value => more verbose)

### Part2: MIRA parameters
#oracle selection method:
#1: "hope"(default)
#2: best metric score(ex: max BLEU)
-oracleSelection	  1 

#prediction selection method:
#1: "fear"(default)
#2: max model score
#3: worst metric score(ex: min BLEU)
-predictionSelection	   1

#shuffle the training samples?(default:1)
-needShuffle 1

#average the weights after each epoch?(default:1)
-needAvg     1

#scale the model score(in order to make it comparable to the metric score)?(default:1)
-needScaling	  1
#options for scaling(only valid when -needScaling=1)
-sentForScaling  0.15   #percentage of the training samples used to estimate the scaling factor(default:0.15)
-scoreRatio	  4	#scale the model score so that abs(model_score/metric_score) \approx scoreRatio(default:4)

#when use BLEU/TER-BLEU as metric, use the pseudo corpus to compute BLEU?(default:1)
-usePseudoCorpus 1
#corpus decay coefficient(only valid when -usePseudoCorpus=1, default:0.99)
-corpusDecay   0.99

#MIRA regularization parameter(default:0.01)
-C    0.01

#run perceptron mode?(default:0)
-runPercep	0

#training mode: can be 1,2,3,4
#1: train dense feature weights only
#2: train dense & sparse feature weights together
#3: train sparse feature weights only(with dense feature weights fixed) also works)
#4: treat sparse features as one component(summary feature), train dense and summary feature weights together
-trainingMode	2

#nbest format: can be "sparse" or "dense"
#for trainingMode 1: either "dense" or "sparse"
#for trainingMode 2-4: use "sparse" format
-nbestFormat	sparse	#dense or sparse
